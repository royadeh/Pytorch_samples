{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet_description.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQgkDlohZpV3tARm/6b9yZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royadeh/Pytorch_samples/blob/main/ResNet_description.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee8MF0FhWI4P"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residual Neural Network (ResNet):\n",
        "What is paper doing? The paper is about ResNet. It was introduced in 2015. it also came first in the ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in the ILSVRC & COCO competitions of 2015.\n",
        "\n",
        "Why is it novel?\n",
        "It has been observed that training the neural networks becomes more difficult with the increase in the number of added layers, and in some cases, the accuracy dwindles as well. The culprit for accuracy degradation was vanishing gradient effect which can only be observed in deeper networks.\n",
        "During the backpropagation stage, the error is calculated and gradient values are determined. The gradients are sent back to hidden layers and the weights are updated accordingly. The process of gradient determination and sending it back to the next hidden layer is continued until the input layer is reached. The gradient becomes smaller and smaller as it reaches the bottom of the network. Therefore, the weights of the initial layers will either update very slowly or remains the same. In other words, the initial layers of the network wonâ€™t learn effectively. Hence, deep network training will not converge and accuracy will either starts to degrade or saturate at a particular value. Although vanishing gradient problem was addressed using the normalized initialization of weights, deeper network accuracy was still not increasing.\n",
        "ResNet wants to solve this problem.\n",
        "\n",
        "Some details about how it is doing it.\n",
        "It has skip connection between every two in 34-layer or every three layers in Resnet50. Large Residual Networks such as 101-layer ResNet101 or ResNet152 are constructed by using more 3-layer blocks. And even at increased network depth, the 152-layer ResNet has much lower complexity (at 11.3bn FLOPS).\n",
        "\n",
        "Result, conclusion and future work:\n",
        "In conclusion, residual network, often known as ResNet, was a significant advancement that altered the training of deep convolutional neural networks for computer vision tasks.\n",
        "While the original Resnet had 34 layers and 2-layer bottleneck blocks, more sophisticated models, such the Resnet50, used 3-layer bottleneck blocks to assure increased accuracy and shorter training times.\n",
        "\n"
      ],
      "metadata": {
        "id": "4KFV_w-XWJtO"
      }
    }
  ]
}